#!/usr/bin/env python3
"""
æ…¢æŸ¥è¯¢åˆ†æè„šæœ¬
ç”¨äºåˆ†ææ•°æ®åº“æ…¢æŸ¥è¯¢å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sqlalchemy import text
from shared.database.connection import get_async_session
from loguru import logger


async def analyze_slow_queries():
    """åˆ†ææ…¢æŸ¥è¯¢"""
    async for db in get_async_session():
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† pg_stat_statements
            check_result = await db.execute(
                text("SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements')")
            )
            has_extension = check_result.scalar()
            
            if not has_extension:
                logger.warning("pg_stat_statements æ‰©å±•æœªå¯ç”¨")
                logger.info("è¦å¯ç”¨ï¼Œè¯·æ‰§è¡Œ: CREATE EXTENSION IF NOT EXISTS pg_stat_statements;")
                return
            
            # è·å–æ…¢æŸ¥è¯¢ç»Ÿè®¡
            query = text("""
                SELECT 
                    query,
                    calls,
                    total_exec_time,
                    mean_exec_time,
                    max_exec_time,
                    stddev_exec_time,
                    rows
                FROM pg_stat_statements
                WHERE mean_exec_time > 100  -- å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡ 100ms
                ORDER BY mean_exec_time DESC
                LIMIT 20
            """)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            if not rows:
                logger.info("âœ… æ²¡æœ‰å‘ç°æ…¢æŸ¥è¯¢ï¼ˆå¹³å‡æ‰§è¡Œæ—¶é—´ > 100msï¼‰")
                return
            
            logger.info(f"ğŸ” å‘ç° {len(rows)} ä¸ªæ…¢æŸ¥è¯¢ï¼š\n")
            
            for i, row in enumerate(rows, 1):
                logger.info(f"\n--- æ…¢æŸ¥è¯¢ #{i} ---")
                logger.info(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {row.mean_exec_time:.2f} ms")
                logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {row.total_exec_time:.2f} ms")
                logger.info(f"æœ€å¤§æ‰§è¡Œæ—¶é—´: {row.max_exec_time:.2f} ms")
                logger.info(f"è°ƒç”¨æ¬¡æ•°: {row.calls}")
                logger.info(f"è¿”å›è¡Œæ•°: {row.rows}")
                logger.info(f"æŸ¥è¯¢: {row.query[:200]}...")
                
                # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                suggestions = []
                if row.calls > 1000:
                    suggestions.append("è€ƒè™‘æ·»åŠ ç¼“å­˜")
                if "WHERE" in row.query and "JOIN" in row.query:
                    suggestions.append("æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç´¢å¼•")
                if row.mean_exec_time > 1000:
                    suggestions.append("è€ƒè™‘ä¼˜åŒ–æŸ¥è¯¢é€»è¾‘æˆ–æ·»åŠ ç´¢å¼•")
                
                if suggestions:
                    logger.info(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: {', '.join(suggestions)}")
            
            # åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
            logger.info("\n\nğŸ“Š ç´¢å¼•ä½¿ç”¨æƒ…å†µåˆ†æï¼š\n")
            index_query = text("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan,
                    idx_tup_read,
                    idx_tup_fetch
                FROM pg_stat_user_indexes
                WHERE idx_scan = 0  -- æœªä½¿ç”¨çš„ç´¢å¼•
                ORDER BY schemaname, tablename
                LIMIT 20
            """)
            
            index_result = await db.execute(index_query)
            unused_indexes = index_result.fetchall()
            
            if unused_indexes:
                logger.warning(f"âš ï¸  å‘ç° {len(unused_indexes)} ä¸ªæœªä½¿ç”¨çš„ç´¢å¼•ï¼š")
                for idx in unused_indexes:
                    logger.warning(f"  - {idx.schemaname}.{idx.tablename}.{idx.indexname}")
                logger.info("ğŸ’¡ å»ºè®®ï¼šè€ƒè™‘åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•ä»¥èŠ‚çœç©ºé—´")
            else:
                logger.info("âœ… æ‰€æœ‰ç´¢å¼•éƒ½åœ¨ä½¿ç”¨ä¸­")
            
        except Exception as e:
            logger.error(f"åˆ†ææ…¢æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
        finally:
            await db.close()
            break


async def analyze_table_statistics():
    """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
    async for db in get_async_session():
        try:
            query = text("""
                SELECT 
                    schemaname,
                    tablename,
                    n_tup_ins as inserts,
                    n_tup_upd as updates,
                    n_tup_del as deletes,
                    n_live_tup as live_rows,
                    n_dead_tup as dead_rows,
                    last_vacuum,
                    last_autovacuum,
                    last_analyze,
                    last_autoanalyze
                FROM pg_stat_user_tables
                WHERE schemaname = 'public'
                ORDER BY n_live_tup DESC
            """)
            
            result = await db.execute(query)
            tables = result.fetchall()
            
            logger.info("\n\nğŸ“ˆ è¡¨ç»Ÿè®¡ä¿¡æ¯ï¼š\n")
            for table in tables:
                logger.info(f"\nè¡¨: {table.tablename}")
                logger.info(f"  è¡Œæ•°: {table.live_rows:,}")
                logger.info(f"  æ­»è¡Œæ•°: {table.dead_rows:,}")
                logger.info(f"  æ’å…¥: {table.inserts:,}")
                logger.info(f"  æ›´æ–°: {table.updates:,}")
                logger.info(f"  åˆ é™¤: {table.deletes:,}")
                
                if table.dead_rows > table.live_rows * 0.1:
                    logger.warning(f"  âš ï¸  æ­»è¡Œæ•°è¾ƒå¤šï¼Œå»ºè®®æ‰§è¡Œ VACUUM")
                
                if not table.last_autovacuum and not table.last_vacuum:
                    logger.warning(f"  âš ï¸  ä»æœªæ‰§è¡Œè¿‡ VACUUM")
                
        except Exception as e:
            logger.error(f"åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        finally:
            await db.close()
            break


if __name__ == "__main__":
    logger.info("ğŸ” å¼€å§‹åˆ†ææ…¢æŸ¥è¯¢...")
    asyncio.run(analyze_slow_queries())
    asyncio.run(analyze_table_statistics())
    logger.info("\nâœ… åˆ†æå®Œæˆï¼")

